{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForQuestionAnswering,\n    TrainingArguments,\n    Trainer,\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-06T11:44:40.715141Z","iopub.execute_input":"2024-12-06T11:44:40.715438Z","iopub.status.idle":"2024-12-06T11:45:01.668892Z","shell.execute_reply.started":"2024-12-06T11:44:40.715410Z","shell.execute_reply":"2024-12-06T11:45:01.667969Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"dataset = load_dataset(\"squad_v2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T11:45:01.670477Z","iopub.execute_input":"2024-12-06T11:45:01.671177Z","iopub.status.idle":"2024-12-06T11:45:04.191801Z","shell.execute_reply.started":"2024-12-06T11:45:01.671136Z","shell.execute_reply":"2024-12-06T11:45:04.191149Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"575c588b85484dd996ce0ff927d70fc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/16.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64bf8572a0324d478397a8c42904a806"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/1.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd194b0134bf41f6891d0ceec9766ea2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/130319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7368a579707d424c8bc322e3efcdce09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/11873 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ebc106a0f4e4b6aa6e5dba7aa99665a"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"print(dataset['train'][1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T11:45:04.192727Z","iopub.execute_input":"2024-12-06T11:45:04.192986Z","iopub.status.idle":"2024-12-06T11:45:04.201050Z","shell.execute_reply.started":"2024-12-06T11:45:04.192961Z","shell.execute_reply":"2024-12-06T11:45:04.200204Z"}},"outputs":[{"name":"stdout","text":"{'id': '56be85543aeaaa14008c9065', 'title': 'BeyoncÃ©', 'context': 'BeyoncÃ© Giselle Knowles-Carter (/biËËˆjÉ’nseÉª/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of BeyoncÃ©\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".', 'question': 'What areas did Beyonce compete in when she was growing up?', 'answers': {'text': ['singing and dancing'], 'answer_start': [207]}}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"model_name = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T19:00:37.752814Z","iopub.execute_input":"2024-12-04T19:00:37.753448Z","iopub.status.idle":"2024-12-04T19:00:40.407301Z","shell.execute_reply.started":"2024-12-04T19:00:37.753411Z","shell.execute_reply":"2024-12-04T19:00:40.406481Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96ee87c9c5d444d288c39c197358d3fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea000162fa694c83b3f69e6ed64ea48d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc1fcaa249894c82937c389302f734e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e145b4c27f5a4e3a8a2a9ef0d5cef508"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"925e6d16914e488b80854b116aa98078"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# 3. Preprocess the data\nmax_length = 512  # Maximum length of input sequences\ndoc_stride = 128  # Helps in splitting long documents","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T19:00:44.121550Z","iopub.execute_input":"2024-12-04T19:00:44.121893Z","iopub.status.idle":"2024-12-04T19:00:44.126067Z","shell.execute_reply.started":"2024-12-04T19:00:44.121863Z","shell.execute_reply":"2024-12-04T19:00:44.125167Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def preprocess_data(examples):\n    # Tokenize the questions and context\n    encoding = tokenizer(\n        examples['question'],\n        examples['context'],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_tensors=\"pt\"\n    )\n\n    # Find the start and end positions of the answer in the context\n    start_positions = []\n    end_positions = []\n\n    for context, answer in zip(examples['context'], examples['answers']):\n        if len(answer['text']) > 0:\n            # Get the text of the first answer (assuming one answer per question)\n            answer_text = answer['text'][0]\n            start_position = context.find(answer_text)\n            \n            # Ensure the answer exists in the context\n            if start_position != -1:\n                end_position = start_position + len(answer_text) - 1\n            else:\n                start_position = 0\n                end_position = 0\n        else:\n            # No valid answer\n            start_position = 0\n            end_position = 0\n\n        start_positions.append(start_position)\n        end_positions.append(end_position)\n\n    # Add start and end positions to the encoding\n    encoding['start_positions'] = start_positions\n    encoding['end_positions'] = end_positions\n\n    return encoding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T19:00:47.882450Z","iopub.execute_input":"2024-12-04T19:00:47.882802Z","iopub.status.idle":"2024-12-04T19:00:47.889365Z","shell.execute_reply.started":"2024-12-04T19:00:47.882770Z","shell.execute_reply":"2024-12-04T19:00:47.888452Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Preprocess both train and validation datasets\ntrain_dataset = dataset[\"train\"].map(preprocess_data, batched=True)\nval_dataset = dataset[\"validation\"].map(preprocess_data, batched=True)\n\n# Remove unnecessary columns after processing\ntrain_dataset = train_dataset.remove_columns([\"question\", \"context\"])\nval_dataset = val_dataset.remove_columns([\"question\", \"context\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T19:00:54.944475Z","iopub.execute_input":"2024-12-04T19:00:54.944820Z","iopub.status.idle":"2024-12-04T19:02:00.172860Z","shell.execute_reply.started":"2024-12-04T19:00:54.944791Z","shell.execute_reply":"2024-12-04T19:02:00.172200Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/130319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1a26900a2be479e983aef93e4aaefcb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11873 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fb89cdff2ae43e2add82ac917487005"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./distilbert-qa\",        # Directory to store the model\n    evaluation_strategy=\"epoch\",         # Evaluate at the end of each epoch\n    learning_rate=2e-5,                  # Learning rate for fine-tuning\n    per_device_train_batch_size=8,       # Adjust to fit your GPU memory\n    per_device_eval_batch_size=8,        # Same as above for evaluation\n    num_train_epochs=3,                  # Number of training epochs\n    save_strategy=\"epoch\",               # Save model at the end of each epoch\n    save_total_limit=2,                  # Limit the number of saved models\n    fp16=True,                           # Enable mixed precision for faster training\n    logging_dir=\"./logs\",                # Directory for logging\n    logging_steps=100,                   # Log every 100 steps\n    report_to=\"none\",                   # Avoid using default WandB or TensorBoard\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T19:02:05.047399Z","iopub.execute_input":"2024-12-04T19:02:05.047761Z","iopub.status.idle":"2024-12-04T19:02:05.193340Z","shell.execute_reply.started":"2024-12-04T19:02:05.047731Z","shell.execute_reply":"2024-12-04T19:02:05.192447Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# 5. Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T19:02:08.361437Z","iopub.execute_input":"2024-12-04T19:02:08.362082Z","iopub.status.idle":"2024-12-04T19:02:08.867574Z","shell.execute_reply.started":"2024-12-04T19:02:08.362048Z","shell.execute_reply":"2024-12-04T19:02:08.866812Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/2415645228.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T19:02:17.033802Z","iopub.execute_input":"2024-12-04T19:02:17.034788Z","iopub.status.idle":"2024-12-04T22:14:07.811100Z","shell.execute_reply.started":"2024-12-04T19:02:17.034750Z","shell.execute_reply":"2024-12-04T22:14:07.810077Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='24435' max='24435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [24435/24435 3:11:48, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.670500</td>\n      <td>2.979045</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.410200</td>\n      <td>2.904812</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3.207600</td>\n      <td>2.943974</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=24435, training_loss=3.5678972463215546, metrics={'train_runtime': 11510.4471, 'train_samples_per_second': 33.965, 'train_steps_per_second': 2.123, 'total_flos': 5.107974402921062e+16, 'train_loss': 3.5678972463215546, 'epoch': 3.0})"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# Evaluate the model on the validation dataset\neval_results = trainer.evaluate()\n\nprint(\"Evaluation Results:\")\nfor key, value in eval_results.items():\n    print(f\"{key}: {value}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T22:14:43.453513Z","iopub.execute_input":"2024-12-04T22:14:43.453847Z","iopub.status.idle":"2024-12-04T22:16:28.840468Z","shell.execute_reply.started":"2024-12-04T22:14:43.453819Z","shell.execute_reply":"2024-12-04T22:16:28.839723Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='743' max='743' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [743/743 01:45]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation Results:\neval_loss: 2.943974018096924\neval_runtime: 105.378\neval_samples_per_second: 112.671\neval_steps_per_second: 7.051\nepoch: 3.0\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"test_data = {\n    \"question\": 'When did Beyonce start becoming popular?',\n    \"context\": 'BeyoncÃ© Giselle Knowles-Carter (/biËËˆjÉ’nseÉª/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of BeyoncÃ©\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".'\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T22:17:17.562565Z","iopub.execute_input":"2024-12-04T22:17:17.562919Z","iopub.status.idle":"2024-12-04T22:17:17.567489Z","shell.execute_reply.started":"2024-12-04T22:17:17.562885Z","shell.execute_reply":"2024-12-04T22:17:17.566605Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"inputs = tokenizer(\n    test_data[\"question\"],\n    test_data[\"context\"],\n    truncation=True,\n    padding=\"max_length\",\n    max_length=max_length,\n    return_tensors=\"pt\"\n).to(\"cuda\")  # Send the data to GPU if available","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T22:17:19.757523Z","iopub.execute_input":"2024-12-04T22:17:19.757867Z","iopub.status.idle":"2024-12-04T22:17:19.763323Z","shell.execute_reply.started":"2024-12-04T22:17:19.757836Z","shell.execute_reply":"2024-12-04T22:17:19.762437Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Get the model's predictions\nmodel.eval()  # Set the model to evaluation mode\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# Extract the start and end logits\nstart_logits = outputs.start_logits\nend_logits = outputs.end_logits\n\n# Get the most probable start and end positions\nstart_index = torch.argmax(start_logits, dim=1).item()\nend_index = torch.argmax(end_logits, dim=1).item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T22:17:26.977968Z","iopub.execute_input":"2024-12-04T22:17:26.978719Z","iopub.status.idle":"2024-12-04T22:17:27.008287Z","shell.execute_reply.started":"2024-12-04T22:17:26.978681Z","shell.execute_reply":"2024-12-04T22:17:27.007644Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForQuestionAnswering,\n    TrainingArguments,\n    Trainer,\n)\ndataset = load_dataset(\"squad_v2\")\nmodel_name = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\n\n# 3. Preprocess the data\nmax_length = 512  # Maximum length of input sequences\ndoc_stride = 128  # Helps in splitting long documents\n\ndef preprocess_data(examples):\n    # Tokenize the questions and context\n    encoding = tokenizer(\n        examples['question'],\n        examples['context'],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_tensors=\"pt\"\n    )\n\n    # Find the start and end positions of the answer in the context\n    start_positions = []\n    end_positions = []\n\n    for context, answer in zip(examples['context'], examples['answers']):\n        if len(answer['text']) > 0:\n            # Get the text of the first answer (assuming one answer per question)\n            answer_text = answer['text'][0]\n            start_position = context.find(answer_text)\n            \n            # Ensure the answer exists in the context\n            if start_position != -1:\n                end_position = start_position + len(answer_text) - 1\n            else:\n                start_position = 0\n                end_position = 0\n        else:\n            # No valid answer\n            start_position = 0\n            end_position = 0\n\n        start_positions.append(start_position)\n        end_positions.append(end_position)\n\n    # Add start and end positions to the encoding\n    encoding['start_positions'] = start_positions\n    encoding['end_positions'] = end_positions\n\n    return encoding\n\n# Preprocess both train and validation datasets\ntrain_dataset = dataset[\"train\"].map(preprocess_data, batched=True)\nval_dataset = dataset[\"validation\"].map(preprocess_data, batched=True)\n\n# Remove unnecessary columns after processing\ntrain_dataset = train_dataset.remove_columns([\"question\", \"context\"])\nval_dataset = val_dataset.remove_columns([\"question\", \"context\"])\n\ntraining_args = TrainingArguments(\n    output_dir=\"./distilbert-qa\",        # Directory to store the model\n    evaluation_strategy=\"epoch\",         # Evaluate at the end of each epoch\n    learning_rate=2e-5,                  # Learning rate for fine-tuning\n    per_device_train_batch_size=8,       # Adjust to fit your GPU memory\n    per_device_eval_batch_size=8,        # Same as above for evaluation\n    num_train_epochs=3,                  # Number of training epochs\n    save_strategy=\"epoch\",               # Save model at the end of each epoch\n    save_total_limit=2,                  # Limit the number of saved models\n    fp16=True,                           # Enable mixed precision for faster training\n    logging_dir=\"./logs\",                # Directory for logging\n    logging_steps=100,                   # Log every 100 steps\n    report_to=\"none\",                   # Avoid using default WandB or TensorBoard\n)\n\n# 5. Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n)\n\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install diffusers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T08:59:40.780863Z","iopub.execute_input":"2024-12-05T08:59:40.781174Z","iopub.status.idle":"2024-12-05T08:59:52.259600Z","shell.execute_reply.started":"2024-12-05T08:59:40.781150Z","shell.execute_reply":"2024-12-05T08:59:52.258403Z"}},"outputs":[{"name":"stdout","text":"Collecting diffusers\n  Downloading diffusers-0.31.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from diffusers) (7.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from diffusers) (3.15.1)\nRequirement already satisfied: huggingface-hub>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from diffusers) (0.26.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from diffusers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from diffusers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from diffusers) (2.32.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from diffusers) (0.4.5)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from diffusers) (10.3.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (2024.6.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (6.0.2)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (4.12.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->diffusers) (3.19.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (2024.6.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.23.2->diffusers) (3.1.2)\nDownloading diffusers-0.31.0-py3-none-any.whl (2.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: diffusers\nSuccessfully installed diffusers-0.31.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms, datasets\nfrom diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler\nfrom transformers import CLIPTextModel, CLIPTokenizer\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Download and prepare CIFAR10 dataset\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),  # Resizing to match the model's input size\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5]),  # Normalize to [-1, 1]\n])\n\ndataset = datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\ntrain_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n\n# Model components\nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\").to(device)\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\").to(device)\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cpu\")  # Offloaded to CPU\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n# Noise scheduler\nnoise_scheduler = DDPMScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\n\n# Optimizer\noptimizer = torch.optim.AdamW(unet.parameters(), lr=5e-5)\n\n# Mixed precision setup\nfrom torch.cuda.amp import GradScaler, autocast\nscaler = GradScaler()\n\n# Training Loop\nepochs = 3  # Reduced number of epochs for faster training\naccumulation_steps = 4  # For gradient accumulation\n\nfor epoch in range(epochs):\n    for i, (images, _) in enumerate(train_loader):\n        # CIFAR10 images don't have captions; use a default placeholder caption\n        captions = [\"A colorful object.\"] * images.size(0)\n        images = images.to(device)\n\n        with autocast():\n            # Preprocess text input\n            inputs = tokenizer(captions, return_tensors=\"pt\", padding=True).to(device)\n            text_embeds = text_encoder(**inputs).last_hidden_state\n\n            # Add noise to the image\n            noise = torch.randn_like(images)\n            timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (1,), device=device).long()\n            noisy_image = noise_scheduler.add_noise(images, noise, timesteps)\n\n            # Predict noise using UNet\n            noise_pred = unet(noisy_image, timesteps, encoder_hidden_states=text_embeds).sample\n\n            # Compute loss\n            loss = torch.nn.functional.mse_loss(noise_pred, noise) / accumulation_steps\n\n        # Backpropagation\n        scaler.scale(loss).backward()\n        if (i + 1) % accumulation_steps == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n\n        print(f\"Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item()}\")\n\n# Save the fine-tuned model\nunet.save_pretrained(\"path_to_save_fine_tuned_unet\")\nvae.save_pretrained(\"path_to_save_fine_tuned_vae\")\ntext_encoder.save_pretrained(\"path_to_save_fine_tuned_text_encoder\")\ntokenizer.save_pretrained(\"path_to_save_fine_tuned_tokenizer\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T09:34:50.177370Z","iopub.execute_input":"2024-12-05T09:34:50.178174Z","iopub.status.idle":"2024-12-05T09:34:50.274213Z","shell.execute_reply.started":"2024-12-05T09:34:50.178125Z","shell.execute_reply":"2024-12-05T09:34:50.272951Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Dataset and DataLoader\u001b[39;00m\n\u001b[1;32m     38\u001b[0m image_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath_to_your_images\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your image directory\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCustomImageDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Model components\u001b[39;00m\n","Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36mCustomImageDataset.__init__\u001b[0;34m(self, image_dir, transform)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_dir \u001b[38;5;241m=\u001b[39m image_dir\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_paths \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_dir, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_your_images'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'path_to_your_images'","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}